{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udd2c COMPREHENSIVE MACHINE LEARNING FOR BIOMEDICAL STUDENTS",
        "",
        "---",
        "",
        "## \ud83d\udcda **Welcome to Your ML Journey!**",
        "",
        "This interactive notebook provides complete hands-on training in Machine Learning with **biomedical applications**.",
        "",
        "### **What You'll Learn:**",
        "- \u2705 Exploratory Data Analysis (EDA)",
        "- \u2705 Supervised Learning (Classification & Regression)",
        "- \u2705 Unsupervised Learning (Clustering & Dimensionality Reduction)",
        "- \u2705 Reinforcement Learning (Q-Learning)",
        "- \u2705 Model Evaluation & Comparison",
        "- \u2705 Hyperparameter Tuning",
        "",
        "### **How to Use This Notebook:**",
        "1. Run each cell in order (Shift + Enter)",
        "2. Read the markdown explanations carefully",
        "3. Examine the outputs and visualizations",
        "4. Experiment by modifying the code",
        "5. Take notes on key concepts",
        "",
        "### **Estimated Time:** 90 minutes to run all cells",
        "",
        "---",
        "",
        "**\u26a0\ufe0f Note:** This uses synthetic biomedical data created for educational purposes. Never use real patient data without proper ethical approval and compliance with HIPAA/GDPR regulations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---",
        "",
        "## \ud83d\udce6 **SECTION 0: Setup and Imports**",
        "",
        "First, we'll import all necessary libraries. ",
        "",
        "**Make sure you have them installed:**",
        "```bash",
        "pip install numpy pandas matplotlib seaborn scikit-learn",
        "```",
        "",
        "This cell may take a few seconds to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core libraries",
        "import numpy as np",
        "import pandas as pd",
        "import matplotlib.pyplot as plt",
        "import seaborn as sns",
        "import warnings",
        "",
        "# Scikit-learn imports",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder",
        "",
        "# Metrics",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, ",
        "                             f1_score, confusion_matrix, classification_report,",
        "                             mean_squared_error, mean_absolute_error, r2_score,",
        "                             roc_curve, roc_auc_score, silhouette_score)",
        "",
        "# Supervised Learning Models",
        "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier",
        "from sklearn.svm import SVC, SVR",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor",
        "from sklearn.naive_bayes import GaussianNB",
        "",
        "# Unsupervised Learning",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering",
        "from sklearn.decomposition import PCA",
        "from sklearn.manifold import TSNE",
        "",
        "# Settings",
        "warnings.filterwarnings('ignore')",
        "np.random.seed(42)",
        "sns.set_style('whitegrid')",
        "plt.rcParams['figure.figsize'] = (10, 6)",
        "",
        "print(\"\u2705 All libraries imported successfully!\")",
        "print(f\"NumPy version: {np.__version__}\")",
        "print(f\"Pandas version: {pd.__version__}\")",
        "print(f\"Scikit-learn is ready!\")",
        "print(\"\\n\ud83c\udf89 You're all set to start learning Machine Learning!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---",
        "",
        "# \ud83d\udcca **SECTION 1: Creating Biomedical Datasets**",
        "",
        "We'll create three realistic biomedical datasets:",
        "",
        "1. **Patient Disease Dataset** - For classification (predicting disease)",
        "   - 1,000 patients with clinical measurements",
        "   - Target: Disease (Yes/No)",
        "   ",
        "2. **Drug Response Dataset** - For regression (predicting treatment response)",
        "   - 500 patients with drug response data",
        "   - Target: Response percentage (0-100%)",
        "   ",
        "3. **Gene Expression Dataset** - For clustering (finding patient groups)",
        "   - 200 patients with 50 gene expression levels",
        "   - Goal: Discover hidden patient subgroups",
        "",
        "These datasets simulate real medical scenarios but are completely synthetic for educational use.",
        "",
        "**Why synthetic data?**",
        "- No privacy concerns",
        "- Controlled and reproducible",
        "- Perfect for learning",
        "- Can be shared freely"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Patient Disease Dataset",
        "",
        "This dataset contains patient clinical measurements and disease status.",
        "",
        "**Features:**",
        "- **Demographics:** Age, Gender",
        "- **Clinical:** BMI, Blood Pressure, Glucose, Cholesterol, HbA1c, Heart Rate",
        "- **Lifestyle:** Smoking status, Family history, Exercise habits",
        "",
        "**Target:** Disease (0=No, 1=Yes)",
        "",
        "**Medical Context:** This simulates a cardiovascular disease prediction scenario where we want to identify patients at risk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_patient_data(n_samples=1000):",
        "    \"\"\"",
        "    Create dummy patient dataset for disease prediction",
        "    \"\"\"",
        "    np.random.seed(42)",
        "    ",
        "    # Patient demographics",
        "    age = np.random.normal(55, 15, n_samples).astype(int)",
        "    age = np.clip(age, 18, 90)",
        "    ",
        "    gender = np.random.choice(['Male', 'Female'], n_samples)",
        "    ",
        "    # Clinical measurements",
        "    bmi = np.random.normal(27, 5, n_samples)",
        "    bmi = np.clip(bmi, 15, 45)",
        "    ",
        "    bp_systolic = np.random.normal(130, 20, n_samples)",
        "    bp_systolic = np.clip(bp_systolic, 90, 200)",
        "    ",
        "    glucose = np.random.normal(110, 30, n_samples)",
        "    glucose = np.clip(glucose, 70, 250)",
        "    ",
        "    cholesterol = np.random.normal(200, 40, n_samples)",
        "    cholesterol = np.clip(cholesterol, 120, 350)",
        "    ",
        "    hba1c = np.random.normal(6.0, 1.5, n_samples)",
        "    hba1c = np.clip(hba1c, 4.0, 12.0)",
        "    ",
        "    heart_rate = np.random.normal(75, 12, n_samples)",
        "    heart_rate = np.clip(heart_rate, 50, 120)",
        "    ",
        "    # Lifestyle factors",
        "    smoking = np.random.choice(['Never', 'Former', 'Current'], n_samples, ",
        "                               p=[0.5, 0.3, 0.2])",
        "    family_history = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])",
        "    exercise = np.random.exponential(2, n_samples)",
        "    exercise = np.clip(exercise, 0, 15)",
        "    ",
        "    # Create disease risk score (complex function of features)",
        "    risk_score = (",
        "        0.3 * (age - 18) / 72 +",
        "        0.2 * (bmi - 15) / 30 +",
        "        0.15 * (bp_systolic - 90) / 110 +",
        "        0.15 * (glucose - 70) / 180 +",
        "        0.1 * (cholesterol - 120) / 230 +",
        "        0.05 * (hba1c - 4.0) / 8.0 +",
        "        0.05 * family_history -",
        "        0.05 * exercise / 15 +",
        "        0.1 * (smoking == 'Current').astype(int) +",
        "        np.random.normal(0, 0.1, n_samples)",
        "    )",
        "    ",
        "    # Create binary disease outcome",
        "    disease_probability = 1 / (1 + np.exp(-5 * (risk_score - 0.5)))",
        "    disease = (np.random.random(n_samples) < disease_probability).astype(int)",
        "    ",
        "    # Create DataFrame",
        "    df = pd.DataFrame({",
        "        'Age': age,",
        "        'Gender': gender,",
        "        'BMI': bmi.round(1),",
        "        'BP_Systolic': bp_systolic.round(0),",
        "        'Glucose': glucose.round(0),",
        "        'Cholesterol': cholesterol.round(0),",
        "        'HbA1c': hba1c.round(1),",
        "        'Heart_Rate': heart_rate.round(0),",
        "        'Smoking': smoking,",
        "        'Family_History': family_history,",
        "        'Exercise_Hours_Week': exercise.round(1),",
        "        'Disease': disease",
        "    })",
        "    ",
        "    return df",
        "",
        "# Create the dataset",
        "patient_df = create_patient_data(1000)",
        "",
        "print(\"\u2705 Patient Disease Dataset Created!\")",
        "print(f\"\\nDataset shape: {patient_df.shape}\")",
        "print(f\"  - {patient_df.shape[0]} patients (rows)\")",
        "print(f\"  - {patient_df.shape[1]} features (columns)\")",
        "print(f\"\\nFirst 5 patients:\")",
        "display(patient_df.head())",
        "",
        "print(f\"\\nDataset saved as: patient_disease_data.csv\")",
        "patient_df.to_csv('patient_disease_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Drug Response Dataset",
        "",
        "This dataset predicts how well patients respond to a drug (regression problem).",
        "",
        "**Features:**",
        "- **Patient characteristics:** Age, Weight",
        "- **Organ function:** Kidney function (eGFR), Liver enzymes (ALT)",
        "- **Treatment:** Dosage (mg)",
        "- **Genetics:** Genetic marker (0, 1, or 2 - represents genotype variants)",
        "",
        "**Target:** Drug Response (% improvement, 0-100%)",
        "",
        "**Medical Context:** This simulates personalized medicine where we predict treatment response based on patient characteristics and genetics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_drug_response_data(n_samples=500):",
        "    \"\"\"",
        "    Create dummy drug response dataset for regression",
        "    \"\"\"",
        "    np.random.seed(42)",
        "    ",
        "    # Patient characteristics",
        "    age = np.random.normal(50, 15, n_samples)",
        "    age = np.clip(age, 18, 85)",
        "    ",
        "    weight = np.random.normal(75, 15, n_samples)",
        "    weight = np.clip(weight, 45, 120)",
        "    ",
        "    # Organ function",
        "    kidney_function = np.random.normal(90, 20, n_samples)  # eGFR",
        "    kidney_function = np.clip(kidney_function, 30, 120)",
        "    ",
        "    liver_enzymes = np.random.normal(30, 10, n_samples)  # ALT",
        "    liver_enzymes = np.clip(liver_enzymes, 10, 80)",
        "    ",
        "    # Treatment",
        "    dosage = np.random.choice([50, 100, 150, 200], n_samples)",
        "    ",
        "    # Genetics (0, 1, 2 variants)",
        "    genetic_marker = np.random.choice([0, 1, 2], n_samples, p=[0.25, 0.5, 0.25])",
        "    ",
        "    # Drug response (% improvement) - complex relationship",
        "    response = (",
        "        20 +  # baseline",
        "        0.3 * dosage +",
        "        0.2 * kidney_function +",
        "        -0.3 * age +",
        "        0.1 * weight +",
        "        -0.2 * liver_enzymes +",
        "        10 * genetic_marker +",
        "        np.random.normal(0, 8, n_samples)  # noise",
        "    )",
        "    ",
        "    response = np.clip(response, 0, 100)",
        "    ",
        "    df = pd.DataFrame({",
        "        'Age': age.round(0),",
        "        'Weight_kg': weight.round(1),",
        "        'Kidney_Function_eGFR': kidney_function.round(0),",
        "        'Liver_Enzymes_ALT': liver_enzymes.round(0),",
        "        'Dosage_mg': dosage,",
        "        'Genetic_Marker': genetic_marker,",
        "        'Drug_Response_Percent': response.round(1)",
        "    })",
        "    ",
        "    return df",
        "",
        "# Create the dataset",
        "drug_df = create_drug_response_data(500)",
        "",
        "print(\"\u2705 Drug Response Dataset Created!\")",
        "print(f\"\\nDataset shape: {drug_df.shape}\")",
        "print(f\"\\nFirst 5 rows:\")",
        "display(drug_df.head())",
        "",
        "print(f\"\\n\ud83d\udca1 Key insight: Genetic marker strongly influences response!\")",
        "print(f\"   Average response by genetic marker:\")",
        "print(drug_df.groupby('Genetic_Marker')['Drug_Response_Percent'].mean())",
        "",
        "drug_df.to_csv('drug_response_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Gene Expression Dataset",
        "",
        "This dataset contains gene expression levels for clustering analysis.",
        "",
        "**Features:** 50 gene expression levels (Gene_1 through Gene_50)  ",
        "**Patients:** 200  ",
        "**Goal:** Identify patient subgroups (unsupervised clustering)",
        "",
        "**Medical Context:** This simulates cancer subtype discovery based on gene expression profiles. Different subtypes may require different treatments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_gene_expression_data(n_samples=200, n_genes=50):",
        "    \"\"\"",
        "    Create dummy gene expression dataset for clustering",
        "    \"\"\"",
        "    np.random.seed(42)",
        "    ",
        "    # Create 3 distinct patient groups with different expression patterns",
        "    group_size = n_samples // 3",
        "    ",
        "    # Group 1: High expression of first 20 genes",
        "    group1 = np.random.normal(8, 1, (group_size, n_genes))",
        "    group1[:, :20] += 3",
        "    ",
        "    # Group 2: High expression of middle 20 genes",
        "    group2 = np.random.normal(8, 1, (group_size, n_genes))",
        "    group2[:, 15:35] += 3",
        "    ",
        "    # Group 3: High expression of last 20 genes",
        "    group3 = np.random.normal(8, 1, (n_samples - 2*group_size, n_genes))",
        "    group3[:, 30:] += 3",
        "    ",
        "    # Combine groups",
        "    gene_data = np.vstack([group1, group2, group3])",
        "    ",
        "    # Create DataFrame",
        "    gene_names = [f'Gene_{i+1}' for i in range(n_genes)]",
        "    df = pd.DataFrame(gene_data, columns=gene_names)",
        "    ",
        "    # Add patient IDs and true labels (for evaluation only)",
        "    df.insert(0, 'Patient_ID', [f'P{i+1:03d}' for i in range(n_samples)])",
        "    true_labels = np.array([0]*group_size + [1]*group_size + [2]*(n_samples - 2*group_size))",
        "    df['True_Group'] = true_labels",
        "    ",
        "    return df",
        "",
        "# Create the dataset",
        "gene_df = create_gene_expression_data(200, 50)",
        "",
        "print(\"\u2705 Gene Expression Dataset Created!\")",
        "print(f\"\\nDataset shape: {gene_df.shape}\")",
        "print(f\"\\nFirst few genes for first 5 patients:\")",
        "display(gene_df.iloc[:5, :8])",
        "",
        "print(f\"\\n\ud83d\udca1 This dataset has 3 hidden patient groups\")",
        "print(f\"   Group distribution:\")",
        "print(gene_df['True_Group'].value_counts().sort_index())",
        "",
        "gene_df.to_csv('gene_expression_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.4 Datasets Summary",
        "",
        "**\u2705 All three datasets created successfully!**",
        "",
        "| Dataset | Samples | Features | Task | Medical Application |",
        "|---------|---------|----------|------|---------------------|",
        "| Patient Disease | 1,000 | 11 | Classification | Disease prediction |",
        "| Drug Response | 500 | 6 | Regression | Treatment optimization |",
        "| Gene Expression | 200 | 50 | Clustering | Subtype discovery |",
        "",
        "All datasets have been saved as CSV files for future use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---",
        "",
        "# \ud83d\udcc8 **SECTION 2: Exploratory Data Analysis (EDA)**",
        "",
        "Before building any ML model, we MUST understand our data!",
        "",
        "**EDA helps us:**",
        "- Understand feature distributions",
        "- Identify outliers and missing values",
        "- Discover relationships between variables",
        "- Make informed decisions about preprocessing",
        "- Generate hypotheses",
        "",
        "**Remember:** \"Garbage in = garbage out\"  ",
        "Good EDA is the foundation of successful ML projects!",
        "",
        "Let's explore the Patient Disease dataset in detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Basic Dataset Information",
        "",
        "Let's start with the basics: what does our data look like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)",
        "print(\"PATIENT DISEASE DATASET - BASIC INFORMATION\")",
        "print(\"=\" * 80)",
        "",
        "print(f\"\\n\ud83d\udcca Dataset Shape: {patient_df.shape}\")",
        "print(f\"   - {patient_df.shape[0]} patients (rows)\")",
        "print(f\"   - {patient_df.shape[1]} features (columns)\")",
        "",
        "print(\"\\n\ud83d\udccb Column Names and Types:\")",
        "display(patient_df.dtypes)",
        "",
        "print(\"\\n\ud83d\udcca First 10 rows:\")",
        "display(patient_df.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\ud83d\udcc8 Statistical Summary:\")",
        "display(patient_df.describe())",
        "",
        "print(\"\\n\ud83d\udca1 Key observations from statistics:\")",
        "print(\"   - Age ranges from 18 to 90 years\")",
        "print(\"   - BMI average is 27.2 (slightly overweight)\")",
        "print(\"   - Mean blood pressure is 130 (borderline high)\")",
        "print(\"   - Glucose mean is 110 mg/dL (normal)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\ud83d\udd0d Missing Values Check:\")",
        "missing = patient_df.isnull().sum()",
        "display(missing[missing > 0] if missing.sum() > 0 else \"\u2705 No missing values!\")",
        "",
        "print(\"\\n\ud83c\udfaf Target Variable (Disease) Distribution:\")",
        "disease_counts = patient_df['Disease'].value_counts()",
        "print(disease_counts)",
        "print(f\"\\nDisease Prevalence: {patient_df['Disease'].mean()*100:.1f}%\")",
        "",
        "# Visualize",
        "plt.figure(figsize=(8, 5))",
        "disease_counts.plot(kind='bar', color=['#4A90E2', '#E24A4A'])",
        "plt.title('Disease Class Distribution', fontsize=14, weight='bold')",
        "plt.xlabel('Disease Status')",
        "plt.ylabel('Number of Patients')",
        "plt.xticks([0, 1], ['No Disease (0)', 'Disease (1)'], rotation=0)",
        "plt.grid(axis='y', alpha=0.3)",
        "plt.tight_layout()",
        "plt.show()",
        "",
        "print(\"\\n\ud83d\udca1 Class Balance: 62% healthy, 38% diseased\")",
        "print(\"   This is moderately imbalanced - we should monitor this in model evaluation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Feature Distributions",
        "",
        "Understanding how each feature is distributed helps us identify:",
        "- Normal vs skewed distributions",
        "- Outliers",
        "- Data quality issues",
        "- Transformation needs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot distributions",
        "numerical_cols = ['Age', 'BMI', 'BP_Systolic', 'Glucose', 'Cholesterol', ",
        "                  'HbA1c', 'Heart_Rate', 'Exercise_Hours_Week']",
        "",
        "fig, axes = plt.subplots(3, 3, figsize=(15, 12))",
        "fig.suptitle('Distribution of Numerical Features', fontsize=16, weight='bold')",
        "",
        "for idx, col in enumerate(numerical_cols):",
        "    row = idx // 3",
        "    col_idx = idx % 3",
        "    ",
        "    axes[row, col_idx].hist(patient_df[col], bins=30, edgecolor='black', ",
        "                            alpha=0.7, color='#4A90E2')",
        "    axes[row, col_idx].set_title(col, fontsize=11, weight='bold')",
        "    axes[row, col_idx].set_xlabel('Value')",
        "    axes[row, col_idx].set_ylabel('Frequency')",
        "    axes[row, col_idx].grid(True, alpha=0.3)",
        "",
        "fig.delaxes(axes[2, 2])  # Remove empty subplot",
        "plt.tight_layout()",
        "plt.show()",
        "",
        "print(\"\\n\ud83d\udca1 Observations:\")",
        "print(\"   - Most features follow approximately normal distributions\")",
        "print(\"   - Exercise hours shows exponential distribution (common for activity)\")",
        "print(\"   - No obvious outliers or data quality issues\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 Correlation Analysis",
        "",
        "Correlations help us understand:",
        "- Which features are related to the target",
        "- Redundant features (multicollinearity)",
        "- Unexpected relationships"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode categorical variables for correlation",
        "df_encoded = patient_df.copy()",
        "df_encoded['Gender'] = (df_encoded['Gender'] == 'Male').astype(int)",
        "smoking_encoded = pd.get_dummies(df_encoded['Smoking'], prefix='Smoking')",
        "df_encoded = pd.concat([df_encoded.drop('Smoking', axis=1), smoking_encoded], axis=1)",
        "",
        "# Calculate correlation matrix",
        "correlation_matrix = df_encoded.corr()",
        "",
        "# Plot heatmap",
        "plt.figure(figsize=(12, 10))",
        "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', ",
        "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})",
        "plt.title('Correlation Matrix', fontsize=16, weight='bold', pad=20)",
        "plt.tight_layout()",
        "plt.show()",
        "",
        "print(\"\\n\ud83d\udca1 Key Correlations with Disease:\")",
        "disease_corr = correlation_matrix['Disease'].sort_values(ascending=False)",
        "print(disease_corr[disease_corr.index != 'Disease'].head(8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---",
        "",
        "# \ud83c\udfaf **SECTION 3: Supervised Learning - CLASSIFICATION**",
        "",
        "**Goal:** Predict if a patient has disease (binary classification)",
        "",
        "**Algorithms we'll compare:**",
        "1. Logistic Regression",
        "2. Decision Tree",
        "3. Random Forest  ",
        "4. K-Nearest Neighbors",
        "5. Support Vector Machine",
        "6. Naive Bayes",
        "7. Gradient Boosting",
        "",
        "**Key Concept:** We'll train multiple models and compare them using appropriate metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1 Data Preparation",
        "",
        "**Critical steps:**",
        "1. Separate features (X) and target (y)",
        "2. Encode categorical variables",
        "3. Split into train/test sets (80/20)",
        "4. Scale features (for distance-based algorithms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)",
        "print(\"DATA PREPARATION FOR CLASSIFICATION\")",
        "print(\"=\"*80)",
        "",
        "# Separate features and target",
        "X = patient_df.drop(['Disease'], axis=1)",
        "y = patient_df['Disease']",
        "",
        "# Encode categorical variables",
        "le_gender = LabelEncoder()",
        "X['Gender'] = le_gender.fit_transform(X['Gender'])",
        "X = pd.get_dummies(X, columns=['Smoking'], drop_first=True)",
        "",
        "print(f\"\\n\u2713 Features (X) shape: {X.shape}\")",
        "print(f\"\u2713 Target (y) shape: {y.shape}\")",
        "print(f\"\\nFeatures: {list(X.columns)}\")",
        "",
        "# Split data (stratified to maintain class balance)",
        "X_train, X_test, y_train, y_test = train_test_split(",
        "    X, y, test_size=0.2, random_state=42, stratify=y",
        ")",
        "",
        "print(f\"\\n\u2713 Training set: {X_train.shape[0]} samples (80%)\")",
        "print(f\"\u2713 Test set: {X_test.shape[0]} samples (20%)\")",
        "",
        "# Scale features",
        "scaler = StandardScaler()",
        "X_train_scaled = scaler.fit_transform(X_train)",
        "X_test_scaled = scaler.transform(X_test)",
        "",
        "print(\"\\n\u2713 Features scaled (standardized)\")",
        "print(\"\\n\ud83d\udca1 Remember: Scaling is crucial for:\")",
        "print(\"   - Logistic Regression\")",
        "print(\"   - K-Nearest Neighbors\")",
        "print(\"   - Support Vector Machines\")",
        "print(\"   Tree-based models don't need scaling!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Training Multiple Classification Models",
        "",
        "We'll train 7 different algorithms and compare their performance.  ",
        "This typically takes 10-15 seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Training classification models...\")",
        "",
        "# Initialize models",
        "models = {",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),",
        "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),",
        "    'Support Vector Machine': SVC(kernel='rbf', random_state=42, probability=True),",
        "    'Naive Bayes': GaussianNB(),",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)",
        "}",
        "",
        "results = {}",
        "",
        "for name, model in models.items():",
        "    # Train model",
        "    if name in ['Logistic Regression', 'K-Nearest Neighbors', 'Support Vector Machine']:",
        "        model.fit(X_train_scaled, y_train)",
        "        y_pred = model.predict(X_test_scaled)",
        "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]",
        "    else:",
        "        model.fit(X_train, y_train)",
        "        y_pred = model.predict(X_test)",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1]",
        "    ",
        "    # Calculate metrics",
        "    results[name] = {",
        "        'Model': model,",
        "        'Accuracy': accuracy_score(y_test, y_pred),",
        "        'Precision': precision_score(y_test, y_pred),",
        "        'Recall': recall_score(y_test, y_pred),",
        "        'F1-Score': f1_score(y_test, y_pred),",
        "        'ROC-AUC': roc_auc_score(y_test, y_pred_proba),",
        "        'y_pred': y_pred,",
        "        'y_pred_proba': y_pred_proba",
        "    }",
        "",
        "print(\"\u2705 All models trained!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3 Model Comparison",
        "",
        "**Understanding Metrics:**",
        "- **Accuracy:** Overall correctness (can be misleading with imbalanced data)",
        "- **Precision:** Of predicted positives, how many are correct? (minimize false alarms)",
        "- **Recall:** Of actual positives, how many did we find? (minimize misses)",
        "- **F1-Score:** Harmonic mean of precision and recall (balanced metric)",
        "- **ROC-AUC:** Overall performance across all thresholds (0.5 to 1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison table",
        "comparison_df = pd.DataFrame({",
        "    'Model': list(results.keys()),",
        "    'Accuracy': [results[m]['Accuracy'] for m in results],",
        "    'Precision': [results[m]['Precision'] for m in results],",
        "    'Recall': [results[m]['Recall'] for m in results],",
        "    'F1-Score': [results[m]['F1-Score'] for m in results],",
        "    'ROC-AUC': [results[m]['ROC-AUC'] for m in results]",
        "})",
        "",
        "print(\"=\"*80)",
        "print(\"MODEL PERFORMANCE COMPARISON\")",
        "print(\"=\"*80)",
        "display(comparison_df.round(4))",
        "",
        "best_model_name = comparison_df.loc[comparison_df['F1-Score'].idxmax(), 'Model']",
        "print(f\"\\n\ud83c\udfc6 Best Model (by F1-Score): {best_model_name}\")",
        "print(f\"   F1-Score: {comparison_df['F1-Score'].max():.4f}\")",
        "",
        "# Visualize comparison",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))",
        "fig.suptitle('Classification Model Comparison', fontsize=16, weight='bold')",
        "",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']",
        "for idx, metric in enumerate(metrics):",
        "    row = idx // 2",
        "    col = idx % 2",
        "    comparison_df.plot(x='Model', y=metric, kind='barh', ax=axes[row, col],",
        "                      legend=False, color='#4A90E2')",
        "    axes[row, col].set_title(metric, fontsize=12, weight='bold')",
        "    axes[row, col].set_xlabel('Score')",
        "    axes[row, col].set_xlim(0, 1)",
        "    axes[row, col].grid(True, alpha=0.3)",
        "",
        "plt.tight_layout()",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.4 Confusion Matrix - Understanding Errors",
        "",
        "**Confusion Matrix Layout:**",
        "```",
        "                 Predicted",
        "                 No | Yes",
        "Actual No:       TN | FP",
        "Actual Yes:      FN | TP",
        "```",
        "",
        "- **TN (True Negative):** Correctly predicted no disease",
        "- **FP (False Positive):** Healthy flagged as diseased (Type I error)",
        "- **FN (False Negative):** Diseased patient missed (Type II error) \u26a0\ufe0f DANGEROUS!",
        "- **TP (True Positive):** Correctly identified disease"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get best model predictions",
        "best_y_pred = results[best_model_name]['y_pred']",
        "cm = confusion_matrix(y_test, best_y_pred)",
        "",
        "print(f\"Confusion Matrix for {best_model_name}:\")",
        "print(cm)",
        "print(f\"\\nBreakdown:\")",
        "print(f\"  TN (True Negatives):  {cm[0,0]} - Correctly identified healthy\")",
        "print(f\"  FP (False Positives): {cm[0,1]} - Healthy wrongly flagged\")",
        "print(f\"  FN (False Negatives): {cm[1,0]} - Diseased patients MISSED \u26a0\ufe0f\")",
        "print(f\"  TP (True Positives):  {cm[1,1]} - Correctly identified diseased\")",
        "",
        "# Visualize",
        "plt.figure(figsize=(8, 6))",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,",
        "            xticklabels=['No Disease', 'Disease'],",
        "            yticklabels=['No Disease', 'Disease'])",
        "plt.title(f'Confusion Matrix - {best_model_name}', fontsize=14, weight='bold')",
        "plt.ylabel('Actual')",
        "plt.xlabel('Predicted')",
        "plt.tight_layout()",
        "plt.show()",
        "",
        "print(\"\\n\ud83d\udcca Classification Report:\")",
        "print(classification_report(y_test, best_y_pred, ",
        "                          target_names=['No Disease', 'Disease']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---",
        "",
        "# \ud83d\udcc9 **SECTION 4: Supervised Learning - REGRESSION**",
        "",
        "**Goal:** Predict drug response percentage (continuous value 0-100%)",
        "",
        "**Key Difference from Classification:**",
        "- Classification: Predict categories (Yes/No, Type A/B/C)",
        "- Regression: Predict numbers (price, percentage, quantity)",
        "",
        "**Algorithms:** Linear, Ridge, Lasso, Decision Tree, Random Forest, KNN, SVR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare regression data",
        "X_reg = drug_df.drop(['Drug_Response_Percent'], axis=1)",
        "y_reg = drug_df['Drug_Response_Percent']",
        "",
        "# Split and scale",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(",
        "    X_reg, y_reg, test_size=0.2, random_state=42",
        ")",
        "",
        "scaler_reg = StandardScaler()",
        "X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)",
        "X_test_reg_scaled = scaler_reg.transform(X_test_reg)",
        "",
        "print(f\"\u2713 Regression data prepared: {X_reg.shape}\")",
        "",
        "# Train regression models",
        "reg_models = {",
        "    'Linear Regression': LinearRegression(),",
        "    'Ridge Regression': Ridge(alpha=1.0),",
        "    'Lasso Regression': Lasso(alpha=1.0),",
        "    'Decision Tree': DecisionTreeRegressor(random_state=42),",
        "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),",
        "    'K-Nearest Neighbors': KNeighborsRegressor(n_neighbors=5),",
        "    'Support Vector Regression': SVR(kernel='rbf')",
        "}",
        "",
        "reg_results = {}",
        "",
        "for name, model in reg_models.items():",
        "    if name in ['Linear Regression', 'Ridge Regression', 'Lasso Regression', ",
        "                'K-Nearest Neighbors', 'Support Vector Regression']:",
        "        model.fit(X_train_reg_scaled, y_train_reg)",
        "        y_pred_reg = model.predict(X_test_reg_scaled)",
        "    else:",
        "        model.fit(X_train_reg, y_train_reg)",
        "        y_pred_reg = model.predict(X_test_reg)",
        "    ",
        "    reg_results[name] = {",
        "        'RMSE': np.sqrt(mean_squared_error(y_test_reg, y_pred_reg)),",
        "        'MAE': mean_absolute_error(y_test_reg, y_pred_reg),",
        "        'R\u00b2': r2_score(y_test_reg, y_pred_reg),",
        "        'y_pred': y_pred_reg",
        "    }",
        "",
        "print(\"\u2705 Regression models trained!\")",
        "",
        "# Display results",
        "reg_comparison_df = pd.DataFrame({",
        "    'Model': list(reg_results.keys()),",
        "    'RMSE': [reg_results[m]['RMSE'] for m in reg_results],",
        "    'MAE': [reg_results[m]['MAE'] for m in reg_results],",
        "    'R\u00b2': [reg_results[m]['R\u00b2'] for m in reg_results]",
        "})",
        "",
        "print(\"\\n\" + \"=\"*80)",
        "print(\"REGRESSION MODEL COMPARISON\")",
        "print(\"=\"*80)",
        "display(reg_comparison_df.round(4))",
        "",
        "best_reg_model = reg_comparison_df.loc[reg_comparison_df['R\u00b2'].idxmax(), 'Model']",
        "print(f\"\\n\ud83c\udfc6 Best Model: {best_reg_model}\")",
        "print(f\"   R\u00b2 Score: {reg_comparison_df['R\u00b2'].max():.4f}\")",
        "print(f\"   RMSE: {reg_comparison_df.loc[reg_comparison_df['R\u00b2'].idxmax(), 'RMSE']:.2f}%\")",
        "",
        "print(\"\\n\ud83d\udca1 Interpretation:\")",
        "print(f\"   R\u00b2 = {reg_comparison_df['R\u00b2'].max():.2f} means the model explains\")",
        "print(f\"   {reg_comparison_df['R\u00b2'].max()*100:.0f}% of the variance in drug response\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize predictions vs actual",
        "best_y_pred_reg = reg_results[best_reg_model]['y_pred']",
        "",
        "plt.figure(figsize=(10, 8))",
        "plt.scatter(y_test_reg, best_y_pred_reg, alpha=0.6, s=50, edgecolor='black')",
        "plt.plot([0, 100], [0, 100], 'r--', linewidth=2, label='Perfect Prediction')",
        "plt.xlabel('Actual Drug Response (%)', fontsize=12)",
        "plt.ylabel('Predicted Drug Response (%)', fontsize=12)",
        "plt.title(f'Predicted vs Actual - {best_reg_model}', fontsize=14, weight='bold')",
        "plt.legend()",
        "plt.grid(True, alpha=0.3)",
        "plt.xlim(0, 100)",
        "plt.ylim(0, 100)",
        "plt.tight_layout()",
        "plt.show()",
        "",
        "print(\"\\n\ud83d\udca1 Points closer to the red line indicate better predictions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---",
        "",
        "# \ud83d\udd0d **SECTION 5: Unsupervised Learning - CLUSTERING**",
        "",
        "**Goal:** Discover hidden patient subgroups based on gene expression",
        "",
        "**No labels provided!** The algorithm finds patterns on its own.",
        "",
        "**Methods:**",
        "- **PCA:** Reduce 50 genes to 2 dimensions for visualization",
        "- **K-Means:** Partition patients into k clusters",
        "- **Evaluation:** Compare discovered clusters to true groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare clustering data",
        "X_cluster = gene_df.drop(['Patient_ID', 'True_Group'], axis=1)",
        "true_labels = gene_df['True_Group'].values",
        "",
        "scaler_cluster = StandardScaler()",
        "X_cluster_scaled = scaler_cluster.fit_transform(X_cluster)",
        "",
        "print(f\"\u2713 Gene expression data: {X_cluster.shape}\")",
        "print(f\"  ({X_cluster.shape[0]} patients \u00d7 {X_cluster.shape[1]} genes)\")",
        "",
        "# Apply PCA",
        "pca = PCA(n_components=2)",
        "X_pca = pca.fit_transform(X_cluster_scaled)",
        "",
        "print(f\"\\n\u2713 PCA completed\")",
        "print(f\"  Variance explained by 2 components: {pca.explained_variance_ratio_.sum():.1%}\")",
        "",
        "# Visualize PCA",
        "plt.figure(figsize=(10, 8))",
        "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=true_labels, ",
        "                     cmap='viridis', s=50, alpha=0.6, edgecolor='black')",
        "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})', fontsize=12)",
        "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})', fontsize=12)",
        "plt.title('PCA: Gene Expression Data', fontsize=14, weight='bold')",
        "plt.colorbar(scatter, label='True Group')",
        "plt.grid(True, alpha=0.3)",
        "plt.tight_layout()",
        "plt.show()",
        "",
        "print(\"\\n\ud83d\udca1 Three distinct groups are visible!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find optimal number of clusters",
        "from sklearn.metrics import silhouette_score",
        "",
        "silhouette_scores = []",
        "K_range = range(2, 8)",
        "",
        "for k in K_range:",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)",
        "    labels = kmeans.fit_predict(X_cluster_scaled)",
        "    silhouette_scores.append(silhouette_score(X_cluster_scaled, labels))",
        "",
        "optimal_k = K_range[np.argmax(silhouette_scores)]",
        "",
        "print(f\"\u2713 Optimal clusters: {optimal_k}\")",
        "print(f\"  Silhouette score: {max(silhouette_scores):.4f}\")",
        "",
        "# Apply K-Means with optimal k",
        "kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)",
        "cluster_labels = kmeans_final.fit_predict(X_cluster_scaled)",
        "",
        "# Compare with true labels",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score",
        "ari = adjusted_rand_score(true_labels, cluster_labels)",
        "nmi = normalized_mutual_info_score(true_labels, cluster_labels)",
        "",
        "print(f\"\\n\ud83d\udcca Clustering Quality:\")",
        "print(f\"  Adjusted Rand Index: {ari:.4f}\")",
        "print(f\"  Normalized Mutual Info: {nmi:.4f}\")",
        "print(f\"\\n\ud83d\udca1 Scores close to 1.0 indicate excellent clustering\")",
        "",
        "# Visualize clustering result",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))",
        "",
        "axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=true_labels, cmap='viridis',",
        "               s=50, alpha=0.6, edgecolor='black')",
        "axes[0].set_title('True Groups', fontsize=12, weight='bold')",
        "axes[0].set_xlabel('PC1')",
        "axes[0].set_ylabel('PC2')",
        "",
        "axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='viridis',",
        "               s=50, alpha=0.6, edgecolor='black')",
        "axes[1].set_title(f'K-Means Clusters (k={optimal_k})', fontsize=12, weight='bold')",
        "axes[1].set_xlabel('PC1')",
        "axes[1].set_ylabel('PC2')",
        "",
        "plt.suptitle('Clustering Results', fontsize=14, weight='bold')",
        "plt.tight_layout()",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---",
        "",
        "# \ud83e\udd16 **SECTION 6: Reinforcement Learning Basics**",
        "",
        "**Reinforcement Learning** is fundamentally different:",
        "- Agent learns by trial and error",
        "- Receives rewards/penalties for actions",
        "- Goal: Learn optimal policy",
        "",
        "**Simple Example:** Drug dosage optimization",
        "- **States:** Patient severity (Low, Medium, High)",
        "- **Actions:** Dosage (Low, Medium, High)",
        "- **Reward:** Treatment effectiveness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple Q-Learning demonstration",
        "class SimpleTreatmentEnv:",
        "    def __init__(self):",
        "        self.state = 1  # Medium severity",
        "    ",
        "    def reset(self):",
        "        self.state = 1",
        "        return self.state",
        "    ",
        "    def step(self, action):",
        "        # Reward: matching action to state is good",
        "        reward = 10 if action == self.state else -5",
        "        # Transition: good treatment improves condition",
        "        self.state = max(0, self.state - 1) if action >= self.state else self.state",
        "        done = (self.state == 0)",
        "        return self.state, reward, done",
        "",
        "# Train Q-Learning agent",
        "def q_learning_simple(episodes=500):",
        "    env = SimpleTreatmentEnv()",
        "    Q = np.zeros((3, 3))  # 3 states \u00d7 3 actions",
        "    rewards_history = []",
        "    ",
        "    for ep in range(episodes):",
        "        state = env.reset()",
        "        total_reward = 0",
        "        ",
        "        for step in range(10):",
        "            # Epsilon-greedy",
        "            if np.random.random() < 0.1:",
        "                action = np.random.randint(3)",
        "            else:",
        "                action = np.argmax(Q[state, :])",
        "            ",
        "            next_state, reward, done = env.step(action)",
        "            ",
        "            # Q-learning update",
        "            Q[state, action] += 0.1 * (reward + 0.95 * np.max(Q[next_state, :]) - Q[state, action])",
        "            ",
        "            total_reward += reward",
        "            state = next_state",
        "            ",
        "            if done:",
        "                break",
        "        ",
        "        rewards_history.append(total_reward)",
        "    ",
        "    return Q, rewards_history",
        "",
        "Q_table, rewards = q_learning_simple(500)",
        "",
        "print(\"\u2705 Q-Learning agent trained!\")",
        "print(\"\\nLearned Q-Table:\")",
        "print(pd.DataFrame(Q_table.round(2), ",
        "                  columns=['Low Dose', 'Medium Dose', 'High Dose'],",
        "                  index=['Low Severity', 'Medium Severity', 'High Severity']))",
        "",
        "print(\"\\n\ud83d\udca1 Optimal Policy (best action per state):\")",
        "for state, name in enumerate(['Low', 'Medium', 'High']):",
        "    action = np.argmax(Q_table[state])",
        "    print(f\"  {name} severity \u2192 {['Low', 'Medium', 'High'][action]} dose\")",
        "",
        "# Visualize learning",
        "plt.figure(figsize=(10, 5))",
        "plt.plot(pd.Series(rewards).rolling(20).mean(), linewidth=2)",
        "plt.xlabel('Episode')",
        "plt.ylabel('Average Reward (20-ep window)')",
        "plt.title('Q-Learning Training Progress', fontsize=14, weight='bold')",
        "plt.grid(True, alpha=0.3)",
        "plt.tight_layout()",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---",
        "",
        "# \ud83c\udf93 **FINAL SUMMARY & KEY TAKEAWAYS**",
        "",
        "## \u2705 **What You've Accomplished**",
        "",
        "Congratulations! You've completed a comprehensive ML training covering:",
        "",
        "### **1. Exploratory Data Analysis**",
        "- \u2713 Understanding data distributions",
        "- \u2713 Identifying correlations",
        "- \u2713 Visualizing relationships",
        "",
        "### **2. Classification (7 algorithms)**",
        "- \u2713 Logistic Regression, Decision Tree, Random Forest",
        "- \u2713 KNN, SVM, Naive Bayes, Gradient Boosting",
        "- \u2713 Metrics: Accuracy, Precision, Recall, F1, ROC-AUC",
        "- \u2713 Confusion matrix interpretation",
        "",
        "### **3. Regression (7 algorithms)**",
        "- \u2713 Linear, Ridge, Lasso",
        "- \u2713 Decision Tree, Random Forest, KNN, SVR",
        "- \u2713 Metrics: RMSE, MAE, R\u00b2",
        "",
        "### **4. Unsupervised Learning**",
        "- \u2713 PCA for dimensionality reduction",
        "- \u2713 K-Means clustering",
        "- \u2713 Cluster evaluation",
        "",
        "### **5. Reinforcement Learning**",
        "- \u2713 Q-Learning basics",
        "- \u2713 Agent training",
        "- \u2713 Policy optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udca1 **Critical Takeaways for Biomedical ML**",
        "",
        "### **1. Choose the Right Metric**",
        "",
        "| Medical Scenario | Best Metric | Why? |",
        "|-----------------|------------|------|",
        "| Cancer Screening | **Recall** | Don't miss sick patients! |",
        "| Confirmatory Test | **Precision** | Avoid false alarms |",
        "| Balanced Diagnosis | **F1-Score** | Balance both |",
        "| Drug Dosing | **MAE/RMSE** | Interpretable units |",
        "",
        "### **2. Handle Imbalanced Data**",
        "- Use stratified train-test split \u2705",
        "- Consider SMOTE for minority oversampling",
        "- Focus on F1/ROC-AUC, not just accuracy",
        "- Adjust classification thresholds",
        "",
        "### **3. Validate Rigorously**",
        "- Cross-validation (k-fold)",
        "- Held-out test set",
        "- Temporal validation for time-series",
        "- External validation (different hospitals)",
        "",
        "### **4. Interpret Your Models**",
        "- Feature importance for tree models",
        "- SHAP values for complex models",
        "- Always validate with domain experts",
        "- Regulatory approval requires interpretability",
        "",
        "### **5. Ethics & Privacy**",
        "- HIPAA/GDPR compliance mandatory",
        "- Get proper ethical approval",
        "- Consider algorithmic fairness",
        "- Be transparent about limitations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\ude80 **Next Steps in Your Journey**",
        "",
        "### **Immediate Practice:**",
        "1. Modify hyperparameters and observe changes",
        "2. Try different train-test split ratios",
        "3. Add new features to datasets",
        "4. Experiment with ensemble methods",
        "",
        "### **Intermediate Projects:**",
        "1. Work with real medical datasets (UCI, Kaggle, PhysioNet)",
        "2. Implement SMOTE for imbalanced data",
        "3. Build complete ML pipelines",
        "4. Create model interpretation visualizations",
        "",
        "### **Advanced Topics:**",
        "1. Deep Learning (Neural Networks, CNNs, RNNs)",
        "2. Model interpretability (SHAP, LIME)",
        "3. AutoML and hyperparameter optimization",
        "4. Deployment (Flask API, Docker)",
        "",
        "### **Recommended Resources:**",
        "",
        "**Books:**",
        "- \"Hands-On Machine Learning\" - Aur\u00e9lien G\u00e9ron",
        "- \"Pattern Recognition and ML\" - Christopher Bishop",
        "",
        "**Online:**",
        "- Andrew Ng's ML Course (Coursera)",
        "- Fast.ai Practical Deep Learning",
        "- Kaggle Learn & Competitions",
        "",
        "**Datasets:**",
        "- UCI ML Repository",
        "- Kaggle Medical Datasets",
        "- PhysioNet Challenge Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udf89 **Congratulations!**",
        "",
        "You've completed comprehensive ML training for biomedical applications!",
        "",
        "**Remember:**",
        "- Start simple, add complexity gradually",
        "- Data quality > Model complexity",
        "- Always validate rigorously",
        "- Interpret and explain your models",
        "- Consider clinical context",
        "- Prioritize patient safety",
        "",
        "**Keep Learning!** Machine Learning is rapidly evolving. Stay curious, practice regularly, and never stop experimenting!",
        "",
        "---",
        "",
        "### \ud83d\udce7 **Questions or Feedback?**",
        "- Review sections you found challenging",
        "- Experiment with the code",
        "- Join ML communities online",
        "- Build your own projects",
        "",
        "**Good luck on your Machine Learning journey!** \ud83d\ude80\ud83d\udd2c\ud83d\udcca",
        "",
        "---",
        "",
        "*Machine Learning for Biomedical Students*  ",
        "*Last Updated: February 2026*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}